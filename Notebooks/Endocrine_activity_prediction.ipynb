{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endocrine activity prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to train and compare the performance of three different architectures on the same task, namely predicting endocrine activity.\n",
    "\n",
    "Endocrine activity is defined as follows:\n",
    "\n",
    "- Active: A molecule is considered active if it shows activity in at least one nuclear receptor assay.\n",
    "- Inactive: A molecule is considered inactive if it does not show activity in any such assay.\n",
    "The detailed process of filtering and curating the assays can be found in the Tox21.ipynb notebook.\n",
    "\n",
    "We overlap these assay results with our current dataset of morphological fingerprints using the corresponding CID (Compound ID). The CID for our library of over $30.000$ compounds was retrieved by querying PubChem. After preprocessing the data, we identified:\n",
    "\n",
    "651 unique molecules with endocrine activity in the raw dataset.\n",
    "642 molecules after preprocessing (based on the overlap between this notebook and the output of the notebook_1).\n",
    "\n",
    "Out of these 642 molecules (signal assay outcome):\n",
    "- 291 are active.\n",
    "- 351 are inactive.\n",
    "\n",
    "Out of these (activity assay outcome)\n",
    "- We have 324 inactive \n",
    "- We have 318 active \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_cid_pubchem = pd.read_pickle('../Data/Annotations/pubchem_annotation1.pkl')\n",
    "#profiles_cid_pubchem = profiles_cid_pubchem[['CID','CPD_NAME']]\n",
    "print(f\"There are {profiles_cid_pubchem['CID'].nunique()} unique compounds in the profiles dataset\")\n",
    "profiles_cid_pubchem.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do : remove dmso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pickle_file ='../Data/CellProfiles/output_notebook_1.pkl'\n",
    "profiles = pickle.load(open(path_to_pickle_file, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(profiles)} profiles in the morphological profiles dataset\")\n",
    "print(f\"There are {profiles['Metadata_broad_sample'].nunique()} unique sample, and {profiles['CPD_NAME'].nunique()} unique compounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of molecules with the same ‘CPD_NAME’ but different stereochemistry (enantiomers), and therefore identified with different ‘Metadata_broad_sample’ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propranolol = profiles[profiles['CPD_NAME']=='propranolol']['Metadata_broad_sample'].to_list()\n",
    "profiles[profiles['CPD_NAME']=='propranolol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As required, now We have different cid for the different 'propranolol' sterochemistry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_cid_pubchem[profiles_cid_pubchem['CPD_NAMES'].isin(propranolol)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_cid_pubchem=profiles_cid_pubchem.rename(columns={\"CPD_NAMES\":\"Metadata_broad_sample\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = profiles_cid_pubchem.merge(profiles, on='Metadata_broad_sample')\n",
    "print(f\"There are {profiles['CID'].nunique()} unique compounds in the activity dataset\")\n",
    "profiles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.read_csv('../Data/Output/Tox21_Endocrine_activity_BBC047.csv', sep=',')\n",
    "activity.rename(columns={'PUBCHEM_CID':'CID','CPD_NAME':'Name'}, inplace=True)\n",
    "print(f\"There are {activity['CID'].nunique()} unique compounds in the activity dataset\")\n",
    "activity.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start with adding the cid columns from profiles_cid_pubchem to our pre-processed morphological profile file -> check which compounds is lost when merging the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = activity.merge(profiles,on='CID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_cmbd = t['CID'].to_list()\n",
    "cid_activity = activity['CID'].to_list()\n",
    "cid_profiles = profiles['CID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cid = list(set(cid_activity) - set(cid_profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(missing_cid)} missing CID in the merge dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = activity.merge(profiles, on='CID')\n",
    "print(f\"We have {profiles['Endocrine_activity'].value_counts()['inactive']} inactive and {profiles['Endocrine_activity'].value_counts()['active']} active compounds\")\n",
    "profiles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiles = profiles[profiles['CPD_NAME'] != 'DMSO'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {profiles['CID'].nunique()},unique CID in the profiles dataset and {profiles['CAS'].nunique()} unique CAS\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data in Training and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML, we always need training and a test set that is pairwise disjoint. <br>\n",
    "To this end, we split the data into two parts, where the training set will contain 80% of the samples and the test set the remaining 20%. The training set will be utilized for training, evaluating, and optimizing the model, while the test set will be used for making predictions<br>\n",
    "\n",
    "Moreover, we need a response vector y, i.e., the column with the activity information and a feature matrix X, i.e., a matrix with the morphological or molecular fingerprints or a combination thereof, for each compound. <br>\n",
    "\n",
    "To make sure we do not have data leakage, we ensure that all instances of compounds, that appear several times in the dataset, are put in the same set. Moreover, we want to ensure that we have enough instances of the active class in both sets. Therefore, we perform a stratified split that ensures a constant ratio of active and inactive compounds in all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import requests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Features and variables to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data,to_remove,activity):\n",
    "    data = data.drop(columns=to_remove)\n",
    "    if data[activity].dtype == 'object':\n",
    "        data[activity] = data[activity].apply(lambda x: 1 if x == 'active' else 0)\n",
    "    X_train, X_test, y_train_activities, y_test_activities= train_test_split(\n",
    "        data.iloc[:,1:], \n",
    "        data[activity].values.astype('int'), \n",
    "        random_state=42, \n",
    "        test_size=0.2, \n",
    "        shuffle=True,\n",
    "        stratify=data[activity].values.astype('int')) #stratify split \n",
    "    return data, X_train, X_test, y_train_activities, y_test_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['CID', 'SMILES', 'CPD_NAME','Metadata_broad_sample', 'CPD_SMILES','CAS','Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, X_train, X_test, y_train_activities, y_test_activities = stratified_split(profiles,to_remove,'Endocrine_activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum( y_test_activities==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Features and variables to predict but make it structural "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to sanitize SMILES strings by keeping the fragment parent, removing ions. Since we have a limited number of data points, we first ensure that all SMILES can be successfully converted into RDKit objects. If any conversion fails, we retrieve the appropriate isomeric structure from PubChem as a fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles_from_cid(cid):\n",
    "    '''\n",
    "    We get smile for cid from pubchem, for molecule where smile is not available when turned into a rdkit object\n",
    "    '''\n",
    "    url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/CanonicalSMILES/JSON\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        smiles = data['PropertyTable']['Properties'][0]['CanonicalSMILES']\n",
    "        return smiles\n",
    "    else:\n",
    "        return f\"Error: Unable to retrieve data for CID {cid}\"\n",
    "\n",
    "def replace_smile(data,old_smiles,new_smile):\n",
    "    '''\n",
    "    Replace the smile queried from pubchem in the dataset\n",
    "    '''\n",
    "    \n",
    "    data.loc[data['SMILES'] == old_smiles, 'SMILES'] = new_smile\n",
    "    return data\n",
    "\n",
    "def check_rdkit_object(dataset,SMILES):\n",
    "    ''' \n",
    "    Check if all smiles can be converted to rdkit object\n",
    "    '''\n",
    "    lg = RDLogger.logger()\n",
    "    lg.setLevel(RDLogger.CRITICAL)\n",
    "    object = dataset[SMILES].apply(Chem.MolFromSmiles)\n",
    "    idx = object[object.isnull()].index\n",
    "    for i in idx:\n",
    "        old_smiles = dataset.loc[i, 'SMILES']\n",
    "        CID = dataset.loc[i, 'CID']\n",
    "        new_smiles = get_smiles_from_cid(CID)\n",
    "        dataset= replace_smile(dataset,old_smiles,new_smiles)\n",
    "    object = dataset[SMILES].apply(Chem.MolFromSmiles)\n",
    "    if object.isnull().sum() == 0:\n",
    "        print('All smiles can be converted to rdkit object')\n",
    "    return dataset\n",
    "\n",
    "def clean_std_smiles(dataset, smiles_column):\n",
    "    \"\"\"\n",
    "    This function standardizes the SMILES strings in the dataset by:\n",
    "    1. Removing salts or solvents.\n",
    "    2. Keeping only the parent molecule.\n",
    "    3. Sanitizing the molecule to ensure it's chemically valid.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_mol = [Chem.MolFromSmiles(smile) for smile in dataset[smiles_column]]\n",
    "    parent_clean_mol = [rdMolStandardize.FragmentParent(mol) for mol in clean_mol]\n",
    "    # Sanitize the parent molecules\n",
    "    for mol in parent_clean_mol:\n",
    "        if mol is not None:\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "            except ValueError as e:\n",
    "                print(f\"Sanitization failed for molecule: {Chem.MolToSmiles(mol)}\")\n",
    "    \n",
    "    # Convert sanitized molecules back to SMILES\n",
    "    smiles = [Chem.MolToSmiles(mol) if mol is not None else None for mol in parent_clean_mol]\n",
    "    dataset['STD_smile'] = smiles\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Get the indices of the training and testing sets to split the structural data in the same way as the morphological data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = data.iloc[X_train.index,0]\n",
    "index_test = data.iloc[X_test.index,0]\n",
    "X_train_struct = profiles.loc[index_train.index, ['SMILES', 'Endocrine_activity','CID']]\n",
    "X_test_struct = profiles.loc[index_test.index, ['SMILES', 'Endocrine_activity','CID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_struct = check_rdkit_object(X_train_struct,'SMILES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_struct = check_rdkit_object(X_test_struct,'SMILES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Standardize smile within each set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Standardized SMILES for training set')\n",
    "X_train_struct = clean_std_smiles(X_train_struct,'SMILES')\n",
    "print('Standardized SMILES for testing set')\n",
    "X_test_struct = clean_std_smiles(X_test_struct,'SMILES')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Generate fingerprint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement two functions : \n",
    "- One to instantiate Morgan fingerprints generator; _morgan_bin_\n",
    "- One that call and computes the binary morgan fingerprint and happened it to the data frame; _make_fps_\n",
    "- One to converts the structure fps features that are stored in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morgan_bin(fpg,dataset):\n",
    "    \n",
    "    '''\n",
    "    Generate Morgan fingerprints as binary vector  \n",
    "        Parameters : \n",
    "            fpg (object): rdFingerprintGenerator(radius=X,fpSize=X) object generated by make_fps()\n",
    "            dataset (data frame): data frame having a column named 'STD_smile' with smiles\n",
    "        Returns : \n",
    "            dataset (data frame): same data frame with one more column named 'morganB_fps' within a list\n",
    "    '''     \n",
    "    RDLogger.DisableLog('rdApp.info')\n",
    "    circular_bit_fp = [fpg.GetFingerprint(Chem.MolFromSmiles(smiles)).ToList() for smiles in dataset['STD_smile']]\n",
    "    dataset['morganB_fps'] =  circular_bit_fp\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def make_fps(dataset,val_radius=2,n_bits=1024):\n",
    "    \n",
    "    '''\n",
    "    Make fingerprints is the function call to create structural fingerprints, combine morphological fingerprints etc \n",
    "        Parameters : \n",
    "            dataset_profiles (data frame): initial dataset having the morphological profiles  \n",
    "            val_radius (int):  default = 2 can be changed, used to make morgan fingerprint cf GetMorganGenerator() Rdkit function \n",
    "            n_bits (int): default = 1024 can be changed, used to make morgan fingerprint cf GetMorganGenerator() Rdkit function \n",
    "        Returns : \n",
    "            dataset (data frame): initial dataset with a supplementary column containing the newly generated fps -> to drop before running make_models for a different fps \n",
    "    ''' \n",
    "    RDLogger.DisableLog('rdApp.info')\n",
    "    fpg = rdFingerprintGenerator.GetMorganGenerator(radius=val_radius, fpSize=n_bits)\n",
    "    dataset = morgan_bin(fpg,dataset)\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def convert_list_features_to_numpy(x):\n",
    "    '''\n",
    "        converts the features that are stored in an array containing lists to an array of arrays such that shape works\n",
    "\n",
    "        @param x: array containing lists \n",
    "        @return x_new: array of arrays of ints\n",
    "    '''\n",
    "    new_x = []\n",
    "    for element in x:\n",
    "        new_x.append(np.array(element))\n",
    "    return np.array(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train_struct.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "morgan_bin(rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024),test)\n",
    "X_train_struct_fps = convert_list_features_to_numpy(test['morganB_fps'].values)\n",
    "morgan_bin(rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024),X_test_struct)\n",
    "X_test_struct_fps = convert_list_features_to_numpy(X_test_struct['morganB_fps'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_list_features_to_numpy(test['morganB_fps'].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_array_to_df(array):\n",
    "    '''\n",
    "    Converts an array of arrays to a pandas dataframe\n",
    "\n",
    "    @param array: array of fps\n",
    "    '''\n",
    "    num_columns = array.shape[1]\n",
    "    column_names = [f'mfp_{i+1}' for i in range(num_columns)]\n",
    "    fingerprint_df = pd.DataFrame(array, columns=column_names)\n",
    "    return fingerprint_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_train_df = convert_array_to_df(X_train_struct_fps)\n",
    "fingerprint_test_df = convert_array_to_df(X_test_struct_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualisation of the training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualise the chemical and morphological space of the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_activity_set(X_train, X_test, y_train_activities, y_test_activities,features_type):\n",
    "    '''\n",
    "    Plot the t-SNE projection of either the chemical or morphological space of the compounds \n",
    "    in the training and test sets. To see if both sets are overlapping in both spaces.\n",
    "    We will change the seed in the stratified split until we get a good overlap of the two sets. \n",
    "    '''\n",
    "\n",
    "    # Add 'activity' column to X_train using assign()\n",
    "    Train = X_train.assign(activity=y_train_activities)\n",
    "    Test = X_test.assign(activity=y_test_activities)\n",
    "\n",
    "    #Add a set column to Train and Test\n",
    "    Train['set'] = ['train'] * len(X_train)\n",
    "    Test['set'] = ['test'] * len(X_test)\n",
    "\n",
    "    # Combine Train and Test \n",
    "    data_combined = pd.concat([Train, Test])\n",
    "\n",
    "    # Fit tsne on the features of the combined data\n",
    "    tsne = TSNE(n_components=2, random_state=52)\n",
    "    projection = tsne.fit_transform(data_combined.iloc[:,:-1])\n",
    "    # Retrieve the projection coordinated for visualization and the labels\n",
    "    projections_df = pd.DataFrame(projection, columns=['x', 'y'])\n",
    "    projections_df['activity'] = data_combined['activity'].values.astype('str')\n",
    "    projections_df['set'] = data_combined['set'].values\n",
    "\n",
    "    # scatter plot with Plotly\n",
    "    fig = px.scatter(\n",
    "    projections_df, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    color='activity',  \n",
    "    symbol='set',\n",
    "    color_discrete_sequence=[px.colors.qualitative.Dark2[4], px.colors.qualitative.Dark2[3]],\n",
    "    labels={'color': 'Activity', 'symbol': 'Set'},\n",
    "    title=\"T-SNE Plot with Train and Test Sets Overlaid of\" + ' ' + features_type + ' ' + 'fingerprint.',\n",
    "    )\n",
    "    fig.show()\n",
    "    return projections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_activity_set(X_train, X_test, y_train_activities, y_test_activities, \"Morphological\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_activity_set(fingerprint_train_df, fingerprint_test_df, y_train_activities, y_test_activities, \"Structrural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi layer perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import  balanced_accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation and Hyperparemeter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune RF using random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_cross_validation(X_train, y_train, max_depth_range=[10, 20, 30, 50], num_tree_range=[100, 200, 300, 500], min_samples_leaf_range=[5,10, 15, 20, 25]):\n",
    "    '''\n",
    "        performs a 5-fold CV for a Random Forest for given X_train and y_train\n",
    "\n",
    "        @param X_train: the training matrix\n",
    "        @param y_train: the associated response vector\n",
    "        @param max_depth_range: list containing the values that should be tested for max depth, default [10,20,30]\n",
    "        @param num_tree_range: list containing the values that should be tested for the number of trees, default [100,300,500]\n",
    "        @param min_samples_leaf_range: list containing the values that should be tested for the minimum number of samples per leaf, default [10,15,20]\n",
    "\n",
    "        @return: a forest with the best hyperparameter according to the estimated test MSE and trained on the whole training set\n",
    "    '''\n",
    "    best_score = -float('inf')\n",
    "    for depth in max_depth_range:\n",
    "        cv_results = cross_validate(RandomForestClassifier(random_state=42, max_depth=depth, n_jobs=-1,\n",
    "                                    class_weight='balanced'), X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_depth = depth\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    for n_tree in num_tree_range:\n",
    "        cv_results = cross_validate(RandomForestClassifier(random_state=42, n_estimators=n_tree,\n",
    "                                    n_jobs=-1, class_weight='balanced'), X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_tree = n_tree\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    for num_samples in min_samples_leaf_range:\n",
    "        cv_results = cross_validate(RandomForestClassifier(random_state=42, min_samples_leaf=num_samples,\n",
    "                                    n_jobs=-1, class_weight='balanced'), X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_min_samples = num_samples\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators=best_n_tree, max_depth=best_depth,\n",
    "                                min_samples_leaf=best_min_samples, n_jobs=-1, class_weight='balanced')\n",
    "    \n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_morpho = rf_cross_validation(X_train, y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_struct = rf_cross_validation(X_train_struct_fps, y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune MLPC using random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPC_cross_validation(X_train, y_train, \n",
    "                        hidden_layer_sizes = [50,150,250, 350, 450, 550], \n",
    "                        activations = ['logistic', 'tanh', 'relu'],\n",
    "                        solvers =  ['adam', 'sgd'],\n",
    "                        alphas = [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "                        learning_rate = ['constant', 'adaptive'],\n",
    "                        max_iter = [150, 400, 600, 800],\n",
    "                        batch_size = [80, 120, 180], \n",
    "                      ):\n",
    "    '''\n",
    "        performs a 5-fold CV for a Random Forest for given X_train and y_train\n",
    "\n",
    "        @param X_train: the training matrix\n",
    "        @param y_train: the associated response vector\n",
    "        @param max_depth_range: list containing the values that should be tested for max depth, default [10,20,30]\n",
    "        @param num_tree_range: list containing the values that should be tested for the number of trees, default [100,300,500]\n",
    "        @param min_samples_leaf_range: list containing the values that should be tested for the minimum number of samples per leaf, default [10,15,20]\n",
    "\n",
    "        @return: a forest with the best hyperparameter according to the estimated test MSE and trained on the whole training set\n",
    "    '''\n",
    "    best_score = -float('inf')\n",
    "    for layer in hidden_layer_sizes :\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42,hidden_layer_sizes=layer),\n",
    "                                      n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_layer =  layer\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    for a in alphas:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, alpha = a), n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_alpha_ = a\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    for solver_ in solvers:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, solver = solver_), n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_solver = solver_\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for activation_ in activations:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, activation= activation_),n_jobs=1,\n",
    "                                     X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_activation= activation_\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for batch in batch_size:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, batch_size= batch), n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_batch_size= batch\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for iter in max_iter:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, max_iter = iter), n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_nb_iter= iter\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    for lr in learning_rate:\n",
    "        cv_results = cross_validate(MLPClassifier(random_state=42, learning_rate = lr), n_jobs=1, X=X_train, y=y_train, scoring='balanced_accuracy', cv=5)\n",
    "        score = np.mean(cv_results['test_score'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "\n",
    "    MLPC = MLPClassifier(random_state  = 42,\n",
    "                         hidden_layer_sizes = best_n_layer,\n",
    "                         alpha = best_alpha_,\n",
    "                         solver = best_solver,\n",
    "                         activation = activation_,\n",
    "                         batch_size = best_batch_size,\n",
    "                         max_iter= best_nb_iter,\n",
    "                         learning_rate= best_lr\n",
    "                         )\n",
    "    \n",
    "    return MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_morpho_mlpc = MLPC_cross_validation(X_train,y_train_activities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_struct_mlpc = MLPC_cross_validation(X_train_struct_fps,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = np.concatenate([X_train.values,fingerprint_train_df.values],axis=1)\n",
    "combined_test = np.concatenate([X_test.values,fingerprint_test_df.values],axis=1)\n",
    "best_estimator_combined = rf_cross_validation(combined_train, y_train_activities)\n",
    "best_estimator_combined_mlpc = MLPC_cross_validation(combined_train, y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RF\"         : [RandomForestClassifier(random_state=42,class_weight ='balanced')], ##default\n",
    "    \"RF_morpho\"  : [best_estimator_morpho] ,\n",
    "    \"RF_struct\"  : [best_estimator_struct],\n",
    "    \"RF_combined\": [best_estimator_combined],\n",
    "\n",
    "    \"MLPC\"       : [MLPClassifier(random_state=42)],  ##default\n",
    "    \"MLP_morpho\"  : [best_estimator_morpho_mlpc ],\n",
    "    \"MLP_struct\"  : [best_estimator_struct_mlpc],\n",
    "    \"MLP_combined\": [best_estimator_combined_mlpc]\n",
    "}\n",
    "\n",
    "def evaluate_model(y_test,y_pred):\n",
    "    \n",
    "    '''\n",
    "    Compute and asses a model with acc balanced acc and f1 metrics \n",
    "        Parameters : \n",
    "            y_test: test to predict \n",
    "            y_pred: pred made by the model\n",
    "        Returns : \n",
    "            classification report \n",
    "    '''    \n",
    "    \n",
    "    ba = balanced_accuracy_score(y_test,y_pred)\n",
    "    mcc = matthews_corrcoef(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "    print(f'Evaluation of predicition made on Test set : ')\n",
    "    print(f'Metrics to evaluate the model:\\n Balanced accuracy : {ba*100:.2f} % ,\\n MCC : {mcc:.3f}.')\n",
    "    print(f'Summary metrics in a cross table:\\n')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "def make_predictions(models_dict ,selected_model,X_train,X_test,y_test,y_train) :\n",
    "\n",
    "    '''\n",
    "    Function to call to evaluate the model and do a cross val \n",
    "        Parameters : \n",
    "            models_dict (dictionary): dict of defined architecture and its parameters\n",
    "            selected_model (str): name of the model to choose\n",
    "            y_test : vector to predict either morphological or structural fingerprint or both \n",
    "    '''      \n",
    "\n",
    "            \n",
    "    if selected_model in models_dict :\n",
    "        model = models_dict[selected_model][0]\n",
    "\n",
    "    #learn by fitting the model on the trainset \n",
    "    model.fit(X_train,y_train) \n",
    "    \n",
    "    #compute prediction on test set \n",
    "    predictions = model.predict(X_test)\n",
    "    evaluate_model(y_test,predictions) \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Morphological fingerprint RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_morpho_rf = make_predictions(models,'RF',X_train,X_test,y_test_activities,y_train_activities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_model_rf = make_predictions(models,'RF_morpho',X_train,X_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Morphological fingerprint MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_morpho_MLPC = make_predictions(models,'MLPC',X_train,X_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_MLPC = make_predictions(models,'MLP_morpho',X_train,X_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.B Structural fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_struct_rf = make_predictions(models,'RF',X_train_struct_fps,X_test_struct_fps,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_model_rf = make_predictions(models,'RF_struct',X_train_struct_fps,X_test_struct_fps,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.B Structural fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_morpho_MLPC = make_predictions(models,'MLPC',X_train_struct_fps,X_test_struct_fps,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_MLPC = make_predictions(models,'MLP_struct',X_train_struct_fps,X_test_struct_fps,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before running model on combined fingerprint we will append the structural and morphological fineprrint using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_rf = make_predictions(models,'RF_combined',combined_train,combined_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_rf_default= make_predictions(models,'RF',combined_train,combined_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_MLP = make_predictions(models,'MLP_combined',combined_train,combined_test,y_test_activities,y_train_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model_MLP_default = make_predictions(models,'MLPC',combined_train,combined_test,y_test_activities,y_train_activities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCC_DL_Endocrinology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
